{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:36px\"><b>Model Inference</b></span>\n",
    "\n",
    "Copyright &copy; 2020 Gunawan Lumban Gaol\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language overning permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T09:32:12.171865Z",
     "start_time": "2020-02-12T09:32:12.161865Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import librosa.display as ld\n",
    "import matplotlib.pyplot as plt\n",
    "from pydub.utils import mediainfo\n",
    "from ipywidgets import interact\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from gurih.data.splitter import Splitter\n",
    "from gurih.data.normalizer import AudioNormalizer\n",
    "from gurih.data.transcriptor import ASRTranscriptor\n",
    "from gurih.features.extractor import MFCCFeatureExtractor\n",
    "from gurih.models.model import BaselineASRModel\n",
    "# from gurih.models.utils import CharMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T03:46:26.425018Z",
     "start_time": "2020-02-12T03:46:25.221898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory is set to ../../models/\n",
      "Documentation directory is set to ../../docs/\n",
      "\n",
      "Model: \"BaselineASR_f200_k11_s2_pvalid_nlstm200_ndense29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "the_input (InputLayer)       [(None, 3000, 39)]        0         \n",
      "_________________________________________________________________\n",
      "masking (Masking)            (None, 3000, 39)          0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 1495, 200)         86000     \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 1495, 400)         641600    \n",
      "_________________________________________________________________\n",
      "the_output (TimeDistributed) (None, 1495, 30)          12030     \n",
      "=================================================================\n",
      "Total params: 739,630\n",
      "Trainable params: 739,630\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = BaselineASRModel(input_shape=(3000, 39), vocab_len=29)\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-12T03:46:27.869163Z",
     "start_time": "2020-02-12T03:46:27.847160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model D:\\Data Science Academy (2)\\voice-to-text-bahasa\\models\\BaselineASR_f200_k11_s2_pvalid_nlstm200_ndense29.h5 from disk.\n"
     ]
    }
   ],
   "source": [
    "filename = os.path.abspath(\"../../models/BaselineASR_f200_k11_s2_pvalid_nlstm200_ndense29.h5\") # must provide abs path\n",
    "model.load(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Audio Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T04:27:40.154314Z",
     "start_time": "2020-02-11T04:27:38.549314Z"
    }
   },
   "outputs": [],
   "source": [
    "# Single file\n",
    "# mp3_file = \"blabla.mp3\"\n",
    "# X = [mp3_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T04:27:41.740314Z",
     "start_time": "2020-02-11T04:27:40.157314Z"
    }
   },
   "outputs": [],
   "source": [
    "# Multiple files from a directory\n",
    "input_dir = \"../../dataset/sample/\"\n",
    "mp3_files = glob.glob(input_dir+\"*.mp3\")\n",
    "X = list(mp3_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If audio duration is more than `max_seq_length` of the model, then the audio will be splitted before transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(x, model, sr=16000, force_mono=False):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    \"\"\"\n",
    "    if mediainfo(x)['channels'] > 1:\n",
    "        warnings.warn(\"Performing channel split and transcripting for each channel.\\\n",
    "                       You can force mono process by passing force_mono=True.\")\n",
    "        if force_mono == True:\n",
    "            x_freq = mono_process(x, sr)\n",
    "    else:\n",
    "        x_freq = mono_process(x, sr)\n",
    "    \n",
    "    transcriptor = Transcriptor(model, CharMap.IDX_TO_CHAR_MAP)\n",
    "    y_pred = transcriptor.predict(x)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "def mono_process(x, sr=16000, plot=False):\n",
    "    # Normalize audio\n",
    "    normalizer = AudioNormalizer(sample_rate=sr,\n",
    "                                 mono=True,\n",
    "                                 write_audio_output=False, # don't output normalized audio\n",
    "                                 output_dir=\".\",\n",
    "                                 encode=False) # don't output .json\n",
    "    x_norm = normalizer.fit_transform(x)\n",
    "    \n",
    "    # Split audio\n",
    "    splitter = Splitter(max_frame_length=80000,\n",
    "                        strides=80000,\n",
    "                        padding='same',\n",
    "                        low_memory=True)\n",
    "    x_splitted = splitter.fit_transform(x)\n",
    "\n",
    "    # Extract MFCC Features\n",
    "    mfcc_extractor = MFCCFeatureExtractor(sample_rate=sr,\n",
    "                                          frame_size=0.025,\n",
    "                                          frame_stride=0.01,\n",
    "                                          filter_num=26,\n",
    "                                          cep_num=13,\n",
    "                                          NFFT=512,\n",
    "                                          low_freq=0,\n",
    "                                          high_freq=None,\n",
    "                                          pre_emphasis_coeff=0.97,\n",
    "                                          cep_lifter=22,\n",
    "                                          dct_type=2,\n",
    "                                          dct_norm=\"ortho\",\n",
    "                                          append_energy=True,\n",
    "                                          append_delta=True,\n",
    "                                          low_memory=False,\n",
    "                                          write_output=False,\n",
    "                                          output_dir=\".\")\n",
    "\n",
    "    x_freq = mfcc_extractor.fit_transform(x_splitted)    \n",
    "\n",
    "    # Create figure for visualization\n",
    "    if plot == True:\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        plt.subplot(2, 1, 1)\n",
    "        _ = ld.waveplot(np.asfortranarray(x_norm), sr=sr)\n",
    "        plt.title(\"Normalized Audio\")\n",
    "        plt.subplot(2, 1, 2)\n",
    "        _ = ld.specshow(x_freq, sr=sr)\n",
    "        plt.title(\"MFCC Features Audio\")\n",
    "        plt.show()\n",
    "    \n",
    "    return x_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-10T06:39:25.646433Z",
     "start_time": "2020-02-10T06:38:32.572433Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "701ffd611781497eb233921d56f8856d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='x', options=('../../dataset/sample\\\\2019-09-03_Annisa Rahmawaty_39â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "def plot_sample(x=X):\n",
    "    data, sr = librosa.load(x, mono=False, sr=22050)\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.subplot(3, 1, 1)\n",
    "    _ = librosa.display.waveplot(data, sr=sr)\n",
    "    plt.title(x)\n",
    "    plt.subplot(3, 1, 2)\n",
    "    _ = librosa.display.waveplot(np.asfortranarray(data[0]), sr=sr)\n",
    "    plt.title(\"Operator Side\")\n",
    "    plt.subplot(3, 1, 3)\n",
    "    _ = librosa.display.waveplot(np.asfortranarray(data[1]), sr=sr)\n",
    "    plt.title(\"Client Side\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return ipd.Audio(x)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:gurih] *",
   "language": "python",
   "name": "conda-env-gurih-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
