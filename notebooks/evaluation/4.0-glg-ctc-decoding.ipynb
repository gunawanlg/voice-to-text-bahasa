{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:36px\"><b>CTC Decoding</b></span>\n",
    "\n",
    "Copyright &copy; 2020 Gunawan Lumban Gaol\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language overning permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:18:35.869500Z",
     "start_time": "2020-03-22T15:18:29.806500Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "# from multiprocess import Pool  # uses dill\n",
    "from collections import defaultdict, Counter\n",
    "from string import ascii_lowercase\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from gurih.utils import batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:18:35.877500Z",
     "start_time": "2020-03-22T15:18:35.872500Z"
    }
   },
   "outputs": [],
   "source": [
    "resource_dir = '4.0-glg-ctc-decoding-resources/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prefix Beam Search\n",
    "\n",
    "This borrows example from https://github.com/corticph/prefix-beam-search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:18:35.907500Z",
     "start_time": "2020-03-22T15:18:35.882500Z"
    }
   },
   "outputs": [],
   "source": [
    "def greedy_decoder(ctc):\n",
    "    \"\"\"\n",
    "    Performs greedy decoding (max decoding) on the output of a CTC network.\n",
    "\n",
    "    Args:\n",
    "    ctc (np.ndarray): The CTC output. Should be a 2D array (timesteps x alphabet_size)\n",
    "\n",
    "    Returns:\n",
    "    string: The decoded CTC output.\n",
    "    \"\"\"\n",
    "\n",
    "    alphabet = list(ascii_lowercase) + [' ', '>']\n",
    "    alphabet_size = len(alphabet)\n",
    "\n",
    "    #  collapse repeating characters\n",
    "    arg_max = np.argmax(ctc, axis=1)\n",
    "    repeat_filter = arg_max[1:] != arg_max[:-1]\n",
    "    repeat_filter = np.concatenate([[True], repeat_filter])\n",
    "    collapsed = arg_max[repeat_filter]\n",
    "\n",
    "    # discard blank tokens (the blank is always last in the alphabet)\n",
    "    blank_filter = np.where(collapsed < (alphabet_size - 1))[0]\n",
    "    final_sequence = collapsed[blank_filter]\n",
    "    full_decode = ''.join([alphabet[letter_idx] for letter_idx in final_sequence])\n",
    "\n",
    "    return full_decode[:full_decode.find('>')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:18:35.987500Z",
     "start_time": "2020-03-22T15:18:35.923500Z"
    }
   },
   "outputs": [],
   "source": [
    "def prefix_beam_search(ctc, lm=None, k=25, alpha=0.30, beta=5, prune=0.001):\n",
    "    \"\"\"\n",
    "    Performs prefix beam search on the output of a CTC network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ctc : np.ndarray \n",
    "        The CTC output. Should be a 2D array (timesteps x alphabet_size)\n",
    "    lm : function, [default=None]\n",
    "        Should take as input a string and output a probability\n",
    "    k : int, [default=25]\n",
    "        The beam width. Will keep the 'k' most likely candidates at each timestep\n",
    "    alpha : float, [default=0.30]\n",
    "        The language model weight. Should usually be between 0 and 1.\n",
    "    beta : float, [default=0.5]\n",
    "        The language model compensation term. The higher the 'alpha', the higher the 'beta'.\n",
    "    prune : float, [default=0.001]\n",
    "        Only extend prefixes with chars with an emission probability higher than 'prune'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    string: The decoded CTC output.\n",
    "    \"\"\"\n",
    "\n",
    "    lm = (lambda l: 1) if lm is None else lm # if no LM is provided, just set to function returning 1\n",
    "    W = lambda l: re.findall(r'\\w+[\\s|>]', l)\n",
    "    alphabet = list(ascii_lowercase) + [' ', '>', '%']\n",
    "    F = ctc.shape[1]\n",
    "    ctc = np.vstack((np.zeros(F), ctc)) # just add an imaginative zero'th step (will make indexing more intuitive)\n",
    "    T = ctc.shape[0]\n",
    "\n",
    "    # STEP 1: Initiliazation\n",
    "    O = ''\n",
    "    Pb, Pnb = defaultdict(Counter), defaultdict(Counter)\n",
    "    Pb[0][O] = 1\n",
    "    Pnb[0][O] = 0\n",
    "    A_prev = [O]\n",
    "    # END: STEP 1\n",
    "\n",
    "    # STEP 2: Iterations and pruning\n",
    "    for t in range(1, T):\n",
    "        pruned_alphabet = [alphabet[i] for i in np.where(ctc[t] > prune)[0]]\n",
    "        for l in A_prev:\n",
    "\n",
    "            if len(l) > 0 and l[-1] == '>':\n",
    "                Pb[t][l] = Pb[t - 1][l]\n",
    "                Pnb[t][l] = Pnb[t - 1][l]\n",
    "                continue  \n",
    "\n",
    "            for c in pruned_alphabet:\n",
    "                c_ix = alphabet.index(c)\n",
    "                # END: STEP 2\n",
    "\n",
    "                # STEP 3: “Extending” with a blank\n",
    "                if c == '%':\n",
    "                    Pb[t][l] += ctc[t][-1] * (Pb[t - 1][l] + Pnb[t - 1][l])\n",
    "                # END: STEP 3\n",
    "\n",
    "                # STEP 4: Extending with the end character\n",
    "                else:\n",
    "                    l_plus = l + c\n",
    "                    if len(l) > 0 and c == l[-1]:\n",
    "                        Pnb[t][l_plus] += ctc[t][c_ix] * Pb[t - 1][l]\n",
    "                        Pnb[t][l] += ctc[t][c_ix] * Pnb[t - 1][l]\n",
    "                # END: STEP 4\n",
    "\n",
    "                    # STEP 5: Extending with any other non-blank character and LM constraints\n",
    "                    elif len(l.replace(' ', '')) > 0 and c in (' ', '>'):\n",
    "                        lm_prob = lm(l_plus.strip(' >')) ** alpha\n",
    "                        Pnb[t][l_plus] += lm_prob * ctc[t][c_ix] * (Pb[t - 1][l] + Pnb[t - 1][l])\n",
    "                    else:\n",
    "                        Pnb[t][l_plus] += ctc[t][c_ix] * (Pb[t - 1][l] + Pnb[t - 1][l])\n",
    "                    # END: STEP 5\n",
    "\n",
    "                    # STEP 6: Make use of discarded prefixes\n",
    "                    if l_plus not in A_prev:\n",
    "                        Pb[t][l_plus] += ctc[t][-1] * (Pb[t - 1][l_plus] + Pnb[t - 1][l_plus])\n",
    "                        Pnb[t][l_plus] += ctc[t][c_ix] * Pnb[t - 1][l_plus]\n",
    "                    # END: STEP 6\n",
    "\n",
    "        # STEP 7: Select most probable prefixes\n",
    "        A_next = Pb[t] + Pnb[t]\n",
    "        sorter = lambda l: A_next[l] * (len(W(l)) + 1) ** beta\n",
    "        A_prev = sorted(A_next, key=sorter, reverse=True)[:k]\n",
    "        # END: STEP 7\n",
    "\n",
    "    return A_prev[0].strip('>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Subword Regularization\n",
    "\n",
    "As implemented by Jennifer Drexler and James Glass in [Subword Regularization and Beam Searh Decoding for End-to-End ASR](http://groups.csail.mit.edu/sls/publications/2019/JenniferDrexler_ICASSP-2019.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:18:37.992500Z",
     "start_time": "2020-03-22T15:18:35.991500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁This', '▁is', '▁a', '▁t', 'est']\n",
      "[209, 31, 9, 375, 586]\n",
      "This is a test\n",
      "This is a test\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# train sentencepiece model from `botchan.txt` and makes `m.model` and `m.vocab`\n",
    "# `m.vocab` is just a reference. not used in the segmentation.\n",
    "spm.SentencePieceTrainer.train(f'--input={resource_dir}botchan.txt --model_prefix={resource_dir}m_botchan --vocab_size=2000')\n",
    "\n",
    "# makes segmenter instance and loads the model file (m.model)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(f'{resource_dir}m_botchan.model')\n",
    "\n",
    "# encode: text => id\n",
    "print(sp.encode_as_pieces('This is a test'))\n",
    "print(sp.encode_as_ids('This is a test'))\n",
    "\n",
    "# decode: id => text\n",
    "print(sp.decode_pieces(['▁This', '▁is', '▁a', '▁t', 'est']))\n",
    "print(sp.decode_ids([209, 31, 9, 375, 586]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:18:38.011500Z",
     "start_time": "2020-03-22T15:18:37.997500Z"
    }
   },
   "outputs": [],
   "source": [
    "class SPLM:\n",
    "    def __init__(self, spp, log=True, regularize=True):\n",
    "        self.spp = spp\n",
    "        self.log = log\n",
    "        self.regularize = regularize\n",
    "    \n",
    "    def __call__(self, sentence, **kwargs):\n",
    "        return self.sp_score(sentence, **kwargs)\n",
    "        \n",
    "    def sp_score(self, sentence, l=-1, alpha=0.2):\n",
    "        \"\"\"Score sentence using unigram model of sentencepiece\"\"\"\n",
    "        if self.regularize:\n",
    "            encoded = self.spp.sample_encode_as_ids(sentence, l, alpha)\n",
    "        else:\n",
    "            encoded = self.spp.encode_as_ids(sentence)\n",
    "\n",
    "        score = 0\n",
    "        for idx in encoded:\n",
    "            # return emission log probabilities, so just add them by chain-rule\n",
    "            score += self.spp.GetScore(idx)  \n",
    "\n",
    "        if not self.log:\n",
    "            score = 10 ** score\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:18:38.076500Z",
     "start_time": "2020-03-22T15:18:38.014500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.237340023052399e-43"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splm = SPLM(sp, log=False, regularize=True)\n",
    "splm('this is a test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Parallel Processing on CPU\n",
    "\n",
    "Beam search decoding on ctc matrix requires an awful lot of computation. On `IPython`, we need to store the worker function on a different module in order to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:18:38.214500Z",
     "start_time": "2020-03-22T15:18:38.090500Z"
    }
   },
   "outputs": [],
   "source": [
    "from worker import ctc_beam_search_sp_mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-20T15:21:01.177Z"
    }
   },
   "source": [
    "This is the barebone code in worker.py \n",
    "\n",
    "```python\n",
    "def worker(b, lm):\n",
    "    res = prefix_beam_search(b,\n",
    "                             lm=lm,\n",
    "                             k=100,\n",
    "                             alpha=0.30,\n",
    "                             beta=5,\n",
    "                             prune=0.001)\n",
    "    return res\n",
    "\n",
    "# create the threadpool\n",
    "with Pool(os.cpu_count() - 1) as p:\n",
    "    # schedule one map/worker for each row in the original data\n",
    "    q = p.starmap(worker, ([b for b in examples], splm))\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark\n",
    "\n",
    "Perform benchmark on various algorithm on our model and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:18:38.229500Z",
     "start_time": "2020-03-22T15:18:38.219500Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_example(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        example = pickle.load(f)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:18:38.277500Z",
     "start_time": "2020-03-22T15:18:38.238500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(860, 29) (860, 29) (860, 29)\n"
     ]
    }
   ],
   "source": [
    "example_1 = load_example(resource_dir+\"example_99.p\")\n",
    "example_2 = load_example(resource_dir+\"example_1518.p\")\n",
    "example_3 = load_example(resource_dir+\"example_2002.p\")\n",
    "\n",
    "print(example_1.shape, example_2.shape, example_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:18:38.300500Z",
     "start_time": "2020-03-22T15:18:38.281500Z"
    }
   },
   "outputs": [],
   "source": [
    "example_1 = np.expand_dims(example_1, axis=0)\n",
    "example_2 = np.expand_dims(example_2, axis=0)\n",
    "example_3 = np.expand_dims(example_3, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark by creating 100 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:18:38.334500Z",
     "start_time": "2020-03-22T15:18:38.313500Z"
    }
   },
   "outputs": [],
   "source": [
    "example_1 = np.vstack([example_1]*33)\n",
    "example_2 = np.vstack([example_2]*33)\n",
    "example_3 = np.vstack([example_3]*34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:18:38.359500Z",
     "start_time": "2020-03-22T15:18:38.342500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 860, 29)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = np.vstack([example_1, example_2, example_3])\n",
    "examples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:18:38.387500Z",
     "start_time": "2020-03-22T15:18:38.365500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(860, 29) (860, 29) (860, 29)\n"
     ]
    }
   ],
   "source": [
    "example_1 = load_example(resource_dir+\"example_99.p\")\n",
    "example_2 = load_example(resource_dir+\"example_1518.p\")\n",
    "example_3 = load_example(resource_dir+\"example_2002.p\")\n",
    "\n",
    "print(example_1.shape, example_2.shape, example_3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In following respective order:\n",
    "1. Python numpy greedy\n",
    "2. Python numpy prefix beam search\n",
    "3. Python numpy multiprocessing prefix beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:18:38.400500Z",
     "start_time": "2020-03-22T15:18:38.392500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but no ghoes tor anything else appeared upon the angient wall\n",
      "mister qualter as the apostle of the middle classes and we re glad twelcomed his gospe\n",
      "alloud laugh followed at chunkeys expencs\n"
     ]
    }
   ],
   "source": [
    "for example in [example_1, example_2, example_3]:\n",
    "    res = greedy_decoder(example)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:18:42.447500Z",
     "start_time": "2020-03-22T15:18:38.409500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but no ghoest tor anything else appeared upon the angient walls\n",
      "mister qualter as the apostle of the middle classes and we are glad t welcomed his gospel\n",
      "alloud laugh followed at chunkeys expense\n"
     ]
    }
   ],
   "source": [
    "for example in [example_1, example_2, example_3]:\n",
    "    res = prefix_beam_search(example,\n",
    "                             lm=None,\n",
    "                             k=100,\n",
    "                             alpha=0.30,\n",
    "                             beta=5,\n",
    "                             prune=0.001)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:18:46.943500Z",
     "start_time": "2020-03-22T15:18:42.449500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but noghoestoranything elseappeared upon theagenwalls\n",
      "mister quiteras theirpostle of the middleclasses andweregladwelcomehis gospll\n",
      "loud laugh  followedatchunkeysexpens\n"
     ]
    }
   ],
   "source": [
    "for example in [example_1, example_2, example_3]:\n",
    "    res = prefix_beam_search(example,\n",
    "                             lm=splm,\n",
    "                             k=100,\n",
    "                             alpha=0.30,\n",
    "                             beta=5,\n",
    "                             prune=0.001)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time\n",
    "\n",
    "In following respective order:\n",
    "1. Python numpy greedy\n",
    "2. Python numpy prefix beam search\n",
    "3. C++ tensorflow greedy\n",
    "4. C++ tensorflow prefix beam search\n",
    "5. Python numpy multiprocessing prefix beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:18:46.975500Z",
     "start_time": "2020-03-22T15:18:46.946500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for example in examples:\n",
    "    greedy_decoder(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:21:00.201500Z",
     "start_time": "2020-03-22T15:18:46.980500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for example in examples:\n",
    "    prefix_beam_search(example,\n",
    "                       lm=None,\n",
    "                       k=100,\n",
    "                       alpha=0.30,\n",
    "                       beta=5,\n",
    "                       prune=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:21:00.431500Z",
     "start_time": "2020-03-22T15:21:00.204500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 217 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = tf.nn.ctc_greedy_decoder(np.transpose(examples, [1, 0, 2]),\n",
    "                             [examples.shape[1]]*examples.shape[0],\n",
    "                             merge_repeated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:21:30.573500Z",
     "start_time": "2020-03-22T15:21:00.435500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 30.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = tf.nn.ctc_beam_search_decoder(np.transpose(examples, [1, 0, 2]),\n",
    "                                  [examples.shape[1]]*examples.shape[0],\n",
    "                                  beam_width=100,\n",
    "                                  top_paths=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T15:22:53.094500Z",
     "start_time": "2020-03-22T15:21:30.578500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = ctc_beam_search_sp_mp(examples)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:gurih] *",
   "language": "python",
   "name": "conda-env-gurih-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
