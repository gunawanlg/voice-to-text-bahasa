{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Post-Extraction Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the summary on how the automated post-extraction pipeline on DigitalOcean cloud was done. The scripts can only be run after the feature extraction process has been done (`npz` and `txt`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T08:45:41.230056Z",
     "start_time": "2020-02-14T08:45:41.227685Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Extraction Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eligibility Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to do is to filter the `npz` and `txt` files that meet the criteria for the training. The reason is we want to filter out the extracted files with the defined `transcription_length` and `frame_length`. During this process the broken `npz` files will also be filtered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T09:06:02.124525Z",
     "start_time": "2020-02-14T09:06:02.110270Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_eligibility(origin_dir, destination_dir=\"eligible_files\", lower_fl_bound=0, upper_fl_bound=99999, lower_tl_bound=0, upper_tl_bound=99999, print_metadata=False, write_output=True, output_filename=\"eligibility_list.csv\", copy=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    destination_dir = f\"{origin_dir}/{destination_dir}\"\n",
    "    if not os.path.exists(destination_dir):\n",
    "        os.mkdir(destination_dir)\n",
    "    \n",
    "    # Create an empty array to store the names\n",
    "    eligible_files = []\n",
    "\n",
    "    # Get all of the npz files\n",
    "    npzs = glob.glob(f\"{origin_dir}/*.npz\")\n",
    "    txts = glob.glob(f\"{origin_dir}/*.txt\")\n",
    "\n",
    "    # Store the total and error counts\n",
    "    count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    # Store the tx and feature lengths\n",
    "    transcription_lengths = []\n",
    "    feature_lengths = []\n",
    "    for npz in npzs:\n",
    "        # Error handling in case there are broken npz file\n",
    "        try:\n",
    "            feature_length = np.load(npz)[\"arr_0\"].shape[0]\n",
    "\n",
    "            if lower_fl_bound <= feature_length <= upper_fl_bound:\n",
    "                txt_conversion = f\"{npz[:-4]}.txt\"\n",
    "                \n",
    "                if os.path.exists(txt_conversion):\n",
    "                    transcription_length = 0\n",
    "                    \n",
    "                    with open(txt_conversion, \"r\") as f:\n",
    "                        for line in f:\n",
    "                            transcription_length += len(line)\n",
    "\n",
    "                    if lower_tl_bound < transcription_length < upper_tl_bound:\n",
    "                        eligible_files.append(npz)\n",
    "                        count += 1\n",
    "                        \n",
    "                        if copy:\n",
    "                            shutil.copy(npz, destination_dir)\n",
    "                            shutil.copy(txt_conversion, destination_dir)\n",
    "                        else:\n",
    "                            shutil.move(npz, destination_dir)\n",
    "                            shutil.move(txt_conversion, destination_dir)\n",
    "\n",
    "                        transcription_lengths.append(transcription_length)\n",
    "                        feature_lengths.append(feature_length)\n",
    "        except:\n",
    "            error_count += 1\n",
    "    \n",
    "    mean_transcription_length = np.mean(transcription_lengths)\n",
    "    median_transcription_length = np.median(transcription_lengths)\n",
    "    min_transcription_length = min(transcription_lengths)\n",
    "    max_transcription_length = max(transcription_lengths)\n",
    "    \n",
    "    mean_feature_length = np.mean(feature_lengths)\n",
    "    median_feature_length = np.median(feature_lengths)\n",
    "    min_feature_length = min(feature_lengths)\n",
    "    max_feature_length = max(feature_lengths)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"transcription_length\": transcription_lengths,\n",
    "        \"feature_length\": feature_lengths\n",
    "    })\n",
    "    \n",
    "    output_filename = f\"{origin_dir}/{output_filename}\"\n",
    "    if write_output:\n",
    "        with open(output_filename, \"w\") as f:\n",
    "            f.write(\"ELIGIBILITY_LIST\\n\")\n",
    "            f.write(\"----------------\\n\")\n",
    "            f.write(f\"COUNT: {count}\\n\")\n",
    "            f.write(f\"MEAN TX LENGTH: {mean_transcription_length}\\n\")\n",
    "            f.write(f\"MEDIAN TX LENGTH: {median_transcription_length}\\n\")\n",
    "            f.write(f\"MIN TX LENGTH: {min_transcription_length}\\n\")\n",
    "            f.write(f\"MAX TX LENGTH: {max_transcription_length}\\n\\n\")\n",
    "\n",
    "            f.write(f\"MEAN FEATURE LENGTH: {mean_feature_length}\\n\")\n",
    "            f.write(f\"MEDIAN FEATURE LENGTH: {median_feature_length}\\n\")\n",
    "            f.write(f\"MIN FEATURE LENGTH: {min_feature_length}\\n\")\n",
    "            f.write(f\"MAX FEATURE LENGTH: {max_feature_length}\\n\")\n",
    "            f.write(\"----------------\\n\\n\")\n",
    "\n",
    "            df.to_csv(f, index=False)\n",
    "    \n",
    "    if print_metadata:\n",
    "        print(f\"COUNT: {count}\")\n",
    "        print(f\"MEAN TX LENGTH: {mean_transcription_length}\")\n",
    "        print(f\"MEDIAN TX LENGTH: {median_transcription_length}\")\n",
    "        print(f\"MIN TX LENGTH: {min_transcription_length}\")\n",
    "        print(f\"MAX TX LENGTH: {max_transcription_length}\")\n",
    "        print(f\"---------------------------------------\")\n",
    "        print(f\"MEAN FEATURE LENGTH: {mean_feature_length}\")\n",
    "        print(f\"MEDIAN FEATURE LENGTH: {median_feature_length}\")\n",
    "        print(f\"MIN FEATURE LENGTH: {min_feature_length}\")\n",
    "        print(f\"MAX FEATURE LENGTH: {max_feature_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - Validation - Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in the post-extraction pipeline is to split the eligible files into train, validation and test. This is done by moving the files to their corresponding directories (`train`, `test`, `val`) so they can easily be loaded to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T11:07:47.167015Z",
     "start_time": "2020-02-14T11:07:47.159896Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_train_val_test(origin_dir, destination_dir=\".\", train_ratio=0.6, val_ratio=0.2, test_ratio=0.2):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    if (train_ratio + val_ratio + test_ratio) != 1:\n",
    "        raise ValueError(\"The sum of the ratios didn't add up to 1.\")\n",
    "    \n",
    "    directory_names = [\"train\", \"val\", \"test\"]\n",
    "    for directory_name in directory_names:\n",
    "        if not os.path.exists(directory_name):\n",
    "            os.mkdir(directory_name)\n",
    "            \n",
    "    npzs = glob.glob(f\"{origin_dir}/*.npz\")\n",
    "    txts = glob.glob(f\"{origin_dir}/*.txt\")\n",
    "    \n",
    "    total_file_count = len(npzs)\n",
    "    train_size = round(train_ratio * total_file_count)\n",
    "    val_size = round(val_ratio * total_file_count)\n",
    "    test_size = total_file_count - (train_size + val_size)\n",
    "    \n",
    "    train_files = npzs[:train_size]\n",
    "    val_files = npzs[train_size:-test_size]\n",
    "    test_files = npzs[-test_size:]\n",
    "    \n",
    "    train_val_test_dict = {directory_names[0]: train_files, directory_names[1]: val_files, directory_names[2]: test_files}\n",
    "    \n",
    "    for file_dir, npzs in train_val_test_dict.items():\n",
    "        for npz in npzs:\n",
    "            txt_conversion = f\"{npz[:-4]}.txt\"\n",
    "            shutil.copy(npz, f\"{destination_dir}/{file_dir}\")\n",
    "            shutil.copy(txt_conversion, f\"{destination_dir}/{file_dir}\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third step is to encode the filenams of the npzs and txts into numbers, since the `DataGenerator` can only use those numbers as the input. The reason behind this is to lower the memory consumption (the `DataGenerator` doesn't have to save long `string`names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T11:29:40.355477Z",
     "start_time": "2020-02-14T11:29:40.350508Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode(origin_dir, output_filename=\"encoding.json\"):\n",
    "    # Get all of the npz files\n",
    "    npzs = glob.glob(f\"{origin_dir}/*.npz\")\n",
    "\n",
    "    # Get all of the available texts to compare it with the ones required    \n",
    "    available_txts = set(glob.glob(f\"{origin_dir}/*.txt\"))\n",
    "    required_txts = set([f\"{npz.replace('npz', 'txt')}\" for npz in npzs])\n",
    "\n",
    "    # Assert the files length of the existing txts must be similar\n",
    "    assert len(available_txts.intersection(required_txts)) == len(required_txts)\n",
    "\n",
    "    encoded_dict = {}\n",
    "    date = datetime.today().strftime(\"%Y%m%d\")\n",
    "\n",
    "    for i, npz in enumerate(npzs):\n",
    "        encoded_dict[i] = npz.replace(\".npz\", \"\")\n",
    "        os.rename(npz, f\"{origin_dir}/{i}.npz\")\n",
    "        os.rename(f\"{npz.replace('npz', 'txt')}\", f\"{origin_dir}/{i}.txt\")\n",
    "    \n",
    "    if output_filename == \"encoding.json\":\n",
    "        with open(f\"{origin_dir}/{date}_audio_encoding.json\", \"w\") as f:\n",
    "            json.dump(encoded_dict, f)\n",
    "    else:\n",
    "        with open(f\"{output_filename}\", \"w\") as f:\n",
    "            json.dump(encoded_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloadable Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
