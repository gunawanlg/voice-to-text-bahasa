{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:36px\"><b>Model Inference</b></span>\n",
    "\n",
    "Copyright &copy; 2020 Gunawan Lumban Gaol\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language overning permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T05:55:23.053500Z",
     "start_time": "2020-02-20T05:55:16.343500Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "import yaml\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import librosa.display as ld\n",
    "import matplotlib.pyplot as plt\n",
    "from pydub.utils import mediainfo\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from gurih.data.splitter import Splitter\n",
    "from gurih.data.normalizer import AudioNormalizer\n",
    "from gurih.features.extractor import MFCCFeatureExtractor\n",
    "from gurih.models.model import BaselineASRModel\n",
    "from gurih.models.decoder import CTCDecoder\n",
    "from gurih.models.utils import CharMap\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T05:55:23.609500Z",
     "start_time": "2020-02-20T05:55:23.053500Z"
    }
   },
   "outputs": [],
   "source": [
    "def init(Pipeline, model_idx=None):\n",
    "    \"\"\"\n",
    "    Initialize model in Pipeline.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Pipeline : Pipeline\n",
    "        correcty configured pipeline\n",
    "    model_idx : int\n",
    "        number indicating model step in pipeline\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Pipeline : Pipeline\n",
    "        initialized pipeline\n",
    "    \"\"\"\n",
    "    if model_idx is None:\n",
    "        raise ValueError('Please provide model_idx param indicating model steps in pipeline.')\n",
    "    \n",
    "    model = Pipeline[model_idx]\n",
    "    model.compile()\n",
    "    model.load(model.dir_path)\n",
    "    \n",
    "    return Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T05:55:24.153500Z",
     "start_time": "2020-02-20T05:55:23.609500Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_mfcc(x_freq):\n",
    "    \"\"\"\n",
    "    For visualization, remove frequency array with zero values\n",
    "    on 2nd to last dimension of mfcc because earlier zero pad\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_freq : numpy.ndarray[shape=(frames, n_mfcc)]\n",
    "        mfcc sequence\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    x_freq : list [shape=(:idx_remove, n_mfcc)]\n",
    "        clean mfcc sequence\n",
    "    \"\"\"\n",
    "    check = np.abs(np.round(x_freq))\n",
    "    idx_remove = None\n",
    "    for i in range(check.shape[0], -1, -1):\n",
    "        stop = check[i:, 1:].any()\n",
    "        if stop:\n",
    "            idx_remove = i+1\n",
    "            return x_freq[:idx_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T05:55:24.706500Z",
     "start_time": "2020-02-20T05:55:24.153500Z"
    }
   },
   "outputs": [],
   "source": [
    "def infer(X, Pipeline, sep='\\n', debug=False, plot=False, progressbar=True):\n",
    "    \"\"\"\n",
    "    Create audio transcript for given audio filename.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy.ndarray[shape=(m, )]\n",
    "        numpy array of audio filenames\n",
    "    Pipeline : Pipeline\n",
    "        initialized pipeline\n",
    "    sep : str\n",
    "        separator string for prediction output chunk\n",
    "    debug : bool, [default=False]\n",
    "        if True, print output shape of each pipeline process\n",
    "    plot : bool, [default=False]\n",
    "        if True, will plot normalized audio waveform and frequency\n",
    "        domain features spectrum. Note that by enabling this, the\n",
    "        pipeline will require more memory to store variables for\n",
    "        plotting.\n",
    "    progressbar : bool, [default=False]\n",
    "        if True, will draw progress bar on inference process\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_preds : numpy.ndarray[shape=(m, c)]\n",
    "        audio transcript of given file input, with chunks denoted\n",
    "        by sep parameters\n",
    "    \"\"\"\n",
    "    # Preprocessing Stage\n",
    "    X = Pipeline[0].fit_transform(X)\n",
    "    \n",
    "    y_preds = []\n",
    "    if progressbar is True: X = tqdm(X)\n",
    "    for x in X:\n",
    "        if x.shape[0] != 1 and Pipeline[0].mono is False:\n",
    "            warnings.warn(\"Performing channel split and transcripting\"\n",
    "                          \" for each channel. You can force mono\"\n",
    "                          \" process by passsing mono=True on the\"\n",
    "                          \" first pipeline step.\")\n",
    "        y_pred_c = []\n",
    "        for x_c in x:\n",
    "            x_c = np.expand_dims(x_c, axis=0)\n",
    "            if plot is True: x_c_cache = x_c.copy()  # store normalized waveform\n",
    "            if debug is True: print(x_c.shape)\n",
    "            \n",
    "            # Preprocessing\n",
    "            for pipe in Pipeline[1:3]:\n",
    "                x_c = pipe.fit_transform(x_c)\n",
    "                if debug is True: print(x_c.shape)\n",
    "            if plot is True: x_freq_cache = x_c.copy()  # store frequency features\n",
    "\n",
    "            # Processing (Model)\n",
    "            x_c = Pipeline[3].predict(x_c)\n",
    "            if debug is True: print(x_c.shape)\n",
    "                \n",
    "            # Post-processing\n",
    "            y_pred = Pipeline[4].fit_predict(x_c)\n",
    "            y_pred_str = sep.join([y for y in y_pred])\n",
    "            \n",
    "            if plot is True:\n",
    "                # Create figure for visualization\n",
    "                plt.figure(figsize=(15, 6))\n",
    "                plt.subplot(2, 1, 1)\n",
    "                _ = ld.waveplot(np.asfortranarray(x_c_cache)[0], sr=sr)\n",
    "                plt.title(\"Normalized Audio\")\n",
    "                plt.subplot(2, 1, 2)\n",
    "                _ = ld.specshow(clean_mfcc(x_freq_cache.reshape(-1, x_freq_cache.shape[-1])).T, sr=sr, y_axis='log')\n",
    "                plt.title(\"MFCC Features Audio\")\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            if debug is True: print(y_pred_str+'\\n')\n",
    "            \n",
    "            y_pred_c.append(y_pred_str)\n",
    "            \n",
    "        y_preds.append(y_pred_c)\n",
    "        \n",
    "    return y_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create with Public API\n",
    "\n",
    "Create your own pipeline using available public API from `gurih` packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T04:01:43.295500Z",
     "start_time": "2020-02-20T04:01:42.800500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define base parameters for your pipeline.\n",
    "sr = 16000\n",
    "force_mono = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the pipeline:\n",
    "1. The minimum pipeline should be `AudioNormalizer`, `Splitter`, and `gurih.models.model` instance.\n",
    "2. Depending on the model, you could add extra decoder part to get string output.\n",
    "3. This minimum pipeline will just use the single features (amplitude) from signals as input of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T07:51:38.096614Z",
     "start_time": "2020-02-19T07:51:37.638614Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = [\n",
    "    AudioNormalizer(sample_rate=sr,\n",
    "                    mono=force_mono,\n",
    "                    write_audio_output=False,  # don't output normalized audio\n",
    "                    output_dir=\"\",\n",
    "                    encode=False),  # don't output .json\n",
    "    Splitter(max_frame_length=16000*30+81,  # sample-rate * (sr_norm / sr_mfcc * stride_mfcc) + (sr_mfcc * stride_mfcc / 2 + 1)\n",
    "             strides=16000*30+81,\n",
    "             padding='same',\n",
    "             low_memory=False),\n",
    "    MFCCFeatureExtractor(sample_rate=sr,\n",
    "                         frame_size=0.025,\n",
    "                         frame_stride=0.01,\n",
    "                         filter_num=26,\n",
    "                         cep_num=13,\n",
    "                         NFFT=512,\n",
    "                         low_freq=0,\n",
    "                         high_freq=None,\n",
    "                         pre_emphasis_coeff=0.97,\n",
    "                         cep_lifter=22,\n",
    "                         dct_type=2,\n",
    "                         dct_norm=\"ortho\",\n",
    "                         append_energy=True,\n",
    "                         append_delta=True,\n",
    "                         low_memory=False,\n",
    "                         write_output=False,\n",
    "                         output_dir=\".\"),\n",
    "    BaselineASRModel(input_shape=(3000, 39), \n",
    "                     vocab_len=29, \n",
    "                     training=False,  # pass false to not save model config\n",
    "                     dir_path=\"../../models/Model010b/BaselineASR_f200_k11_s2_pvalid_nlstm200_ndense29.h5\",\n",
    "                     doc_path=\"../../docs/Model010b/\"),\n",
    "    CTCDecoder(CharMap.IDX_TO_CHAR_MAP)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T07:51:41.275614Z",
     "start_time": "2020-02-19T07:51:40.802614Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save created pipeline\n",
    "config_filename = \"../../docs/Model010b/config.yaml\"\n",
    "with open(config_filename, 'w') as f:\n",
    "    f.writelines(yaml.dump(pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Saved Pipeline\n",
    "\n",
    "Optionally, you can also load your `.yaml` preconfigured pipeline file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T04:07:16.851500Z",
     "start_time": "2020-02-20T04:07:16.149000Z"
    }
   },
   "outputs": [],
   "source": [
    "config_filename = \"../../docs/Model010b/config.yaml\"\n",
    "with open(config_filename, 'r') as f:\n",
    "    pipeline = yaml.full_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T04:07:17.361500Z",
     "start_time": "2020-02-20T04:07:16.851500Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Pipeline\n",
    "\n",
    "First, prepare the model by compiling and loading trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T04:07:18.466500Z",
     "start_time": "2020-02-20T04:07:17.364000Z"
    }
   },
   "outputs": [],
   "source": [
    "model_idx = 3\n",
    "pipeline = init(pipeline, model_idx=model_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Audio Transcript\n",
    "\n",
    "Use the pipeline to create audio transcript. You just simply use `infer` method which accepts a `Pipeline` object and a saved model path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T04:16:36.282000Z",
     "start_time": "2020-02-20T04:16:35.789500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Multiple files from a directory\n",
    "input_dir = \"../../dataset/sample/\"\n",
    "mp3_files = glob.glob(input_dir+\"*.mp3\")\n",
    "X = mp3_files\n",
    "print(f\"Total audio files: {len(X)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T04:16:36.764500Z",
     "start_time": "2020-02-20T04:16:36.284500Z"
    }
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create transcription from input audio by following steps:\n",
    "1. If channels > 1, split audio files into each channel\n",
    "2. For each channel, if audio `frames > max_seq_length` then the audio will be splitted before transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T04:24:21.951000Z",
     "start_time": "2020-02-20T04:16:37.479500Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_preds = infer(X, pipeline, sep='\\n', progressbar=True)\n",
    "y_preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Pipeline with Plot\n",
    "\n",
    "Inference pipeline with plot, great for single file usage to inspect more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T04:25:13.605500Z",
     "start_time": "2020-02-20T04:25:13.110500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Single file\n",
    "mp3_file = \"../../dataset/sample/202102_1.mp3\"\n",
    "x = np.array([mp3_file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T04:25:14.475500Z",
     "start_time": "2020-02-20T04:25:13.948000Z"
    }
   },
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create transcription from input audio by following steps:\n",
    "1. If channels > 1, split audio files into each channel\n",
    "2. For each channel, if audio `frames > max_seq_length` then the audio will be splitted before transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T04:26:57.445000Z",
     "start_time": "2020-02-20T04:26:27.228500Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = infer(x, pipeline, sep='\\n', plot=True, progressbar=False)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run below cell to hear what the audio is like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T04:26:58.270000Z",
     "start_time": "2020-02-20T04:26:57.445000Z"
    }
   },
   "outputs": [],
   "source": [
    "ipd.Audio(x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then inspect the output prediction from the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-20T04:27:09.927500Z",
     "start_time": "2020-02-20T04:27:09.415000Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:gurih] *",
   "language": "python",
   "name": "conda-env-gurih-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341.484px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "11469bf3a7fe4b35b69c6803364e09da": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "15df77ba869c4573aa2c229bac3cb359": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2451a77990bd4a4fa17476b3b459d4d1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "2f85a69b96534a14b1ad429a018fb0d1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_738c6ae5faee4a759284cc8dc3dae817",
        "IPY_MODEL_6191ec4cd4804b74ae665e591cf1270a"
       ],
       "layout": "IPY_MODEL_bd6e2591d27e4fc78b365a06bcc1bbba"
      }
     },
     "3d94b5cd7ee1483a86fe0f7d9e995d45": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "6191ec4cd4804b74ae665e591cf1270a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_15df77ba869c4573aa2c229bac3cb359",
       "style": "IPY_MODEL_e9471dca3d3b48b3aa8f4c6acdd682a6",
       "value": " 11/11 [47:26&lt;00:00, 258.78s/it]"
      }
     },
     "677fc4aeb95f46fcaca97ba54db46e83": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "738c6ae5faee4a759284cc8dc3dae817": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "description": "100%",
       "layout": "IPY_MODEL_dd2eb8010bfd47e289541291b4f45428",
       "max": 11,
       "style": "IPY_MODEL_677fc4aeb95f46fcaca97ba54db46e83",
       "value": 11
      }
     },
     "7a1d9ece0ba34ade85437c336737406c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a40892e3fe614e4794f282d2df3002ac",
       "style": "IPY_MODEL_2451a77990bd4a4fa17476b3b459d4d1",
       "value": " 1/1 [39:21&lt;00:00, 2361.26s/it]"
      }
     },
     "a40892e3fe614e4794f282d2df3002ac": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bd6e2591d27e4fc78b365a06bcc1bbba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bea35025af404e9b865710354403b3c2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "description": "100%",
       "layout": "IPY_MODEL_11469bf3a7fe4b35b69c6803364e09da",
       "max": 1,
       "style": "IPY_MODEL_3d94b5cd7ee1483a86fe0f7d9e995d45",
       "value": 1
      }
     },
     "dd2eb8010bfd47e289541291b4f45428": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "dfab99ca6c5e4a858a52c7c2f709df2b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e0b00a595986413bbbea22f57255a095": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_bea35025af404e9b865710354403b3c2",
        "IPY_MODEL_7a1d9ece0ba34ade85437c336737406c"
       ],
       "layout": "IPY_MODEL_dfab99ca6c5e4a858a52c7c2f709df2b"
      }
     },
     "e9471dca3d3b48b3aa8f4c6acdd682a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
